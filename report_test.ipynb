{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9051f558fe7b4f66bcc85cc0b8ec6571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f843639ccbe431e8a69939a3e6d511a",
              "IPY_MODEL_ec7c75fbcedc467d829e0c2125fb6c3b"
            ],
            "layout": "IPY_MODEL_e52602fa060a46ee8b72f0bcc25b8a61"
          }
        },
        "3f843639ccbe431e8a69939a3e6d511a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17dd542a48e343bbb7ee18cf99d7a313",
            "placeholder": "​",
            "style": "IPY_MODEL_6a5ac0daa8a84868aa7ba34dc8653b47",
            "value": " 12.5%"
          }
        },
        "ec7c75fbcedc467d829e0c2125fb6c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4221038b9514b4db8f9a187c4bab271",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e54d782adb5048ab8d96a92dce3967d1",
            "value": 12.522787827794138
          }
        },
        "e52602fa060a46ee8b72f0bcc25b8a61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17dd542a48e343bbb7ee18cf99d7a313": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a5ac0daa8a84868aa7ba34dc8653b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4221038b9514b4db8f9a187c4bab271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e54d782adb5048ab8d96a92dce3967d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Set up**"
      ],
      "metadata": {
        "id": "E_H9Z__HugNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You may replace the workspace directory if you want.\n",
        "workspace_dir = '.'\n",
        "\n",
        "# Training progress bar\n",
        "!pip install -q qqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FRb1eE9etYJ0",
        "outputId": "3716ad05-7957-4bb3-f10d-96f1ad3101b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for qqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WiJWsT6xr5Q4",
        "outputId": "5917e5c0-0a16-450b-e857-c41972919463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1yxo_HLz3Nc-SxQeY61cMtbsVHWzJ8S20\n",
            "From (redirected): https://drive.google.com/uc?id=1yxo_HLz3Nc-SxQeY61cMtbsVHWzJ8S20&confirm=t&uuid=85c16238-b1ae-47de-9480-569f11757c5b\n",
            "To: /content/Data.zip\n",
            "100% 671M/671M [00:13<00:00, 48.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "!gdown --id 1yxo_HLz3Nc-SxQeY61cMtbsVHWzJ8S20 --output \"{workspace_dir}/Data.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"{workspace_dir}/Data.zip\" -d \"{workspace_dir}/\""
      ],
      "metadata": {
        "id": "nRr7e-J5tpDN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import**"
      ],
      "metadata": {
        "id": "FzMb8yLGu2-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "y8M_9TEvu3ap"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperperameters**"
      ],
      "metadata": {
        "id": "TmpT0fq2ovwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 8"
      ],
      "metadata": {
        "id": "tgEiYVGMovEA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocess**"
      ],
      "metadata": {
        "id": "j3QHLz_Lur5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StainDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        root_dir: 包含 'Original' 和 'Stained' 兩個子資料夾的根目錄路徑。\n",
        "        transform: 進行的預處理和增強的轉換。\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.original_path = os.path.join(root_dir, 'Original')\n",
        "        self.stained_path = os.path.join(root_dir, 'Stained')\n",
        "        self.images = os.listdir(self.original_path)  # 假設每個原始圖像都有對應的染色圖像\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        original_img_path = os.path.join(self.original_path, img_name)\n",
        "        stained_img_path = os.path.join(self.stained_path, img_name)\n",
        "\n",
        "        original_image = Image.open(original_img_path)\n",
        "        stained_image = Image.open(stained_img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            original_image = self.transform(original_image)\n",
        "            stained_image = self.transform(stained_image)\n",
        "\n",
        "        return original_image, stained_image\n",
        "\n",
        "# 定義轉換\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # 調整圖像大小\n",
        "    transforms.ToTensor(),  # 轉化為tensor\n",
        "])\n",
        "\n",
        "# 創建數據集\n",
        "dataset = StainDataset(root_dir=f'{workspace_dir}/Data', transform=transform)\n",
        "\n",
        "\n",
        "# 分割train data 和 test data\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "test_size = total_size - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# 創建DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "dwnemriAurdl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generator**"
      ],
      "metadata": {
        "id": "ePVBiKy9xwZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    def conv_bn_LRelu(in_dim, out_dim):\n",
        "      return nn.Sequential(\n",
        "        nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "      )\n",
        "\n",
        "    def deconv_bn_Relu(in_dim, out_dim, output_layer=False):\n",
        "      if output_layer:\n",
        "        return nn.Sequential(\n",
        "          nn.ConvTranspose2d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "          nn.Tanh()\n",
        "        )\n",
        "      return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        nn.ReLU(inplace=True),\n",
        "      )\n",
        "\n",
        "    self.stained_img_encoder = nn.Sequential(\n",
        "      conv_bn_LRelu(3, 64),\n",
        "      conv_bn_LRelu(64, 128),\n",
        "      conv_bn_LRelu(128, 256),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(256 * 32 * 32, 512)\n",
        "    )\n",
        "\n",
        "    self.noise_encoder = nn.Sequential(\n",
        "      nn.Linear(100, 256),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "      nn.Linear(256, 512)\n",
        "    )\n",
        "\n",
        "    self.combined_model = nn.Sequential(\n",
        "      nn.Linear(1024, 256 * 32 * 32),  # 新增的線性層\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Unflatten(1, (256, 32, 32)),  # 重塑為 4D 張量\n",
        "      deconv_bn_Relu(256, 128),\n",
        "      deconv_bn_Relu(128, 64),\n",
        "      deconv_bn_Relu(64, 3, output_layer=True)\n",
        "    )\n",
        "  def forward(self, z, stained_imgs):\n",
        "    encoded_img = self.stained_img_encoder(stained_imgs)\n",
        "    encoded_noise = self.noise_encoder(z)\n",
        "    combined_input = torch.cat([encoded_img, encoded_noise], dim=1)\n",
        "    output = self.combined_model(combined_input)\n",
        "    return output  # 重塑輸出成圖像的尺寸\n"
      ],
      "metadata": {
        "id": "tmwNb5CbxwrN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discriminator**"
      ],
      "metadata": {
        "id": "N49yNhIsx2EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    def conv_bn_lrelu(in_dim, out_dim):\n",
        "      return nn.Sequential(\n",
        "        nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.3)\n",
        "      )\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        # 輸入圖像尺寸為3 x 256 x 256\n",
        "      nn.Conv2d(3, 64, 5, 2, 2),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "      conv_bn_lrelu(64, 128),\n",
        "      conv_bn_lrelu(128, 256),\n",
        "      conv_bn_lrelu(256, 512),\n",
        "      conv_bn_lrelu(512, 1024),\n",
        "      conv_bn_lrelu(1024, 2048),\n",
        "      nn.Conv2d(2048, 1, 4),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, img):\n",
        "    y = self.model(img)\n",
        "    y = y.view(-1, 1)\n",
        "    return y"
      ],
      "metadata": {
        "id": "Qv0B5_0nx2TY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save**"
      ],
      "metadata": {
        "id": "vluvoLHGzIf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image(gen_imgs):\n",
        "    \"\"\"將生成的圖像保存成一張圖\"\"\"\n",
        "    gen_imgs = gen_imgs.view(gen_imgs.size(0), 3, 96, 96).cpu().detach().numpy()\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
        "    cnt = 0\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            axs[i, j].imshow(np.transpose(gen_imgs[cnt], (1, 2, 0)))\n",
        "            axs[i, j].axis('off')\n",
        "            cnt += 1\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sDxFwscGzF17"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize**"
      ],
      "metadata": {
        "id": "EByzixVeyPQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 檢查是否可以使用 GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 初始化Generator Discriminator\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# 初始化Loss function\n",
        "adversarial_loss = torch.nn.BCELoss().to(device)\n",
        "\n",
        "# 設定優化器\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n"
      ],
      "metadata": {
        "id": "ydzAHGFLyqqo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "FNZ27rjqy1nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qqdm.notebook import qqdm\n",
        "# 訓練參數\n",
        "latent_dim = 100  # 噪聲向量的維度\n",
        "sample_interval = 10  # 每隔多少批次儲存一次生成的圖像\n",
        "\n",
        "# 開始訓練\n",
        "for epoch in range(epochs):\n",
        "  progress_bar = qqdm(train_loader)\n",
        "  for i, (real_imgs, stained_imgs) in enumerate(progress_bar):\n",
        "\n",
        "    # 準備真實的圖像並將它們移動到 GPU 上（如果有的話）\n",
        "    real_imgs = real_imgs.to(device)\n",
        "    stained_imgs = stained_imgs.to(device)\n",
        "    batch_size = real_imgs.size(0)\n",
        "    # 創建標籤\n",
        "    valid = torch.ones(batch_size, 1, requires_grad=False).to(device)  # 真實標籤\n",
        "    fake = torch.zeros(batch_size, 1, requires_grad=False).to(device)  # 假標籤\n",
        "\n",
        "    # ---- 訓練生成器 ----\n",
        "    optimizer_G.zero_grad()\n",
        "\n",
        "    # 隨機生成噪聲與stained_imgs串接後輸入到模型中\n",
        "    z = torch.randn(batch_size, latent_dim).to(device)\n",
        "    gen_imgs = generator(z, stained_imgs)\n",
        "\n",
        "    # 計算生成器的損失\n",
        "    g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "    # 反向傳播並更新生成器的權重\n",
        "    g_loss.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    # ---- 訓練鑑別器 ----\n",
        "    optimizer_D.zero_grad()\n",
        "\n",
        "    # 計算鑑別器對真實圖像的損失\n",
        "    real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "\n",
        "    # 計算鑑別器對假圖像的損失\n",
        "    fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "\n",
        "    # 總損失是兩部分損失的和\n",
        "    d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "    # 反向傳播並更新鑑別器的權重\n",
        "    d_loss.backward()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    progress_bar.set_infos({\n",
        "      'Loss_D': round(d_loss.item(), 4),\n",
        "      'Loss_G': round(g_loss.item(), 4),\n",
        "      'Epoch': epoch + 1,\n",
        "    })\n",
        "\n",
        "  # 每隔一定批次打印一次損失\n",
        "\n",
        "  print(f\"Epoch [{epoch + 1}/{epochs}] Batch {i}/{len(train_loader)} \\ Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\")\n",
        "  save_image(gen_imgs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "9051f558fe7b4f66bcc85cc0b8ec6571",
            "3f843639ccbe431e8a69939a3e6d511a",
            "ec7c75fbcedc467d829e0c2125fb6c3b",
            "e52602fa060a46ee8b72f0bcc25b8a61",
            "17dd542a48e343bbb7ee18cf99d7a313",
            "6a5ac0daa8a84868aa7ba34dc8653b47",
            "e4221038b9514b4db8f9a187c4bab271",
            "e54d782adb5048ab8d96a92dce3967d1"
          ]
        },
        "id": "wrbXtpiCy1Fc",
        "outputId": "663f19c9-04f1-4509-f925-8c13459cb51d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[K\u001b[F  \u001b[1mIters\u001b[0m      \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m    \u001b[1mLoss_D\u001b[0m  \u001b[1mLoss_G\u001b[0m  \u001b[1mEpoch\u001b[0m                                       \n",
              " \u001b[99m893/\u001b[93m7131\u001b[0m\u001b[0m  \u001b[99m00:07:24<\u001b[93m00:51:44\u001b[0m\u001b[0m  \u001b[99m2.01it/s\u001b[0m  \u001b[99m0.4737\u001b[0m  \u001b[99m1.0621\u001b[0m    \u001b[99m1\u001b[0m                                         "
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IpythonBar(children=(HTML(value='  0.0%'), FloatProgress(value=0.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9051f558fe7b4f66bcc85cc0b8ec6571"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate**"
      ],
      "metadata": {
        "id": "gysOEtxhzNG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for i, (real_imgs, stained_imgs) in enumerate(test_loader):\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        z = torch.randn(real_imgs.size(0), latent_dim).to(device)\n",
        "        gen_imgs = generator(z, stained_imgs)\n",
        "        save_image(gen_imgs, epoch='Test', batch_i=i)\n",
        "        break  # 測試一個批次\n"
      ],
      "metadata": {
        "id": "_WMdBv8BzNeH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}